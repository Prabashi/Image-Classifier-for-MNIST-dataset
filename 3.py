# -*- coding: utf-8 -*-
"""3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uhfezMm1bMvSJ44lwwODwtHr_m-7qSvo
"""

from keras import models, layers
from keras.datasets import mnist
from keras.utils import to_categorical
import matplotlib.pyplot as plt
import numpy as np
from keras.optimizers import SGD
from keras.constraints import max_norm

# load data
(train_X, train_y), (test_X, test_y) = mnist.load_data()
train_X.shape, test_X.shape

_, ax = plt.subplots(1, 10, figsize=(10,10))

for i in range(0, 10):
    ax[i].axis('off')
    ax[i].imshow(train_X[i], cmap=plt.cm.binary)

model = models.Sequential()
# Set input shape in the first layer
# First convolutional layer with filter size (3,3) and 32 filters followed by a max pooling layer
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))

# Second convolutional layer with filter size (3,3) and 64 filters followed by a max pooling layer
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

#  Flatten to provide features to the classifier
model.add(layers.Flatten())

# Dense layer to interpret the features with 64 nodes
model.add(layers.Dense(64, activation='relu'))

# To handle overfitting
model.add(layers.Dropout(0.2))

# Output layer. Has 10 nodes for 0-9 outputs. Use a softmax activation to find probabilities and assign classes
model.add(layers.Dense(10, activation='softmax'))

# opt = SGD(lr=0.01, momentum=0.9) # Stochastic gradient descent optimizer with a learning rate of 0.01 and a momentum of 0.9
# opt = 'adam'
opt = 'rmsprop'
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

# Preprocessing of the images
# Reshape dataset to have 4D vector with the number of channels of the input image as 1 (since image is grayscale)
train_X = train_X.reshape((60000, 28, 28, 1))
test_X = test_X.reshape((10000, 28, 28, 1))

# Normalize by rescaling pixel values from range [0, 255] to [0, 1] by 1st converting the value to float and then by dividing from max value
train_X= train_X.astype('float32') / 255
test_X= test_X.astype('float32') / 255

# Convert to one-hot encoding
train_y = to_categorical(train_y)
test_y = to_categorical(test_y)

# Add noise to the dataset
noise_factor = 0.25
train_X_noisy = train_X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_X.shape)
test_X_noisy = test_X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=test_X.shape)

train_X_noisy = np.clip(train_X_noisy, 0., 1.)
test_X_noisy = np.clip(test_X_noisy, 0., 1.)

# Create the model for deoising
modelDenoiser = models.Sequential()
modelDenoiser.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
modelDenoiser.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform'))
modelDenoiser.add(layers.Conv2DTranspose(32, kernel_size=(3,3), activation='relu', kernel_initializer='he_uniform'))
modelDenoiser.add(layers.Conv2DTranspose(64, kernel_size=(3,3), activation='relu', kernel_initializer='he_uniform'))
modelDenoiser.add(layers.Conv2D(1, kernel_size=(3, 3), kernel_constraint=max_norm(2.0), activation='sigmoid', padding='same'))

# Compile and fit data
modelDenoiser.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
modelDenoiser.fit(train_X_noisy, train_X, epochs=5, batch_size=64, validation_split=0.2)

# Generate denoised images
train_X_denoised = modelDenoiser.predict(train_X_noisy)
test_X_denoised = modelDenoiser.predict(test_X_noisy)

history = model.fit(train_X_denoised, train_y, epochs=20, batch_size=64, validation_split=0.2)

test_loss, test_acc = model.evaluate(test_X_denoised, test_y)
print('Accuracy:', test_acc)
print('Loss: ', test_loss)